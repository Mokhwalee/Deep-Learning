# üìò Deep Learning Study 

This repository offers a structured deep dive into **Deep Learning**, inspired by [GeeksforGeeks' Deep Learning Tutorial](https://www.geeksforgeeks.org/deep-learning-tutorial/). It includes theoretical explanations and hands-on Jupyter Notebooks for major concepts and architectures in deep learning.

---

## üîç Overview

Deep Learning is a subset of machine learning that uses **neural networks with multiple layers** (deep architectures) to model complex patterns in large datasets. It is particularly powerful in tasks involving images, audio, natural language, and sequential data.

---

## üß± Core Concepts and Notebooks

Each section below includes a Jupyter Notebook demonstrating key ideas, implementation, and experimentation using Python (with frameworks like TensorFlow or PyTorch).

### 1. Introduction to Neural Networks  
**Notebook:** `01_intro_neural_networks.ipynb`  
Explains the basic components: neurons, weights, biases, activation functions, and the architecture of a simple feedforward network.

### 2. Activation Functions  
**Notebook:** `02_activation_functions.ipynb`  
Covers common activation functions such as Sigmoid, Tanh, ReLU, and Softmax, and explains when and why they are used.

### 3. Loss Functions and Optimizers  
**Notebook:** `03_loss_optimizers.ipynb`  
Introduces loss functions (MSE, Cross-Entropy) and optimizers (SGD, Adam, RMSProp), including how they impact training dynamics.

### 4. Forward and Backpropagation  
**Notebook:** `04_backpropagation.ipynb`  
Demonstrates the process of forward pass, error calculation, and weight updates using backpropagation.

### 5. Deep Neural Networks (DNN)  
**Notebook:** `05_deep_neural_network.ipynb`  
Builds a deep feedforward network and discusses depth vs. performance tradeoffs.

### 6. Convolutional Neural Networks (CNN)  
**Notebook:** `06_cnn.ipynb`  
Introduces CNNs for image processing tasks, covering concepts like filters, pooling layers, and convolution operations.

### 7. Recurrent Neural Networks (RNN)  
**Notebook:** `07_rnn.ipynb`  
Explores RNNs for sequential data like time series or text, explaining hidden states and vanishing gradients.

### 8. Long Short-Term Memory (LSTM)  
**Notebook:** `08_lstm.ipynb`  
An improvement over RNNs, LSTMs capture long-term dependencies using gates (input, forget, output).

### 9. Autoencoders  
**Notebook:** `09_autoencoder.ipynb`  
Unsupervised neural networks for dimensionality reduction and data reconstruction.

### 10. Generative Adversarial Networks (GANs)  
**Notebook:** `10_gan.ipynb`  
Covers the adversarial learning framework involving a generator and discriminator, used to generate realistic synthetic data.

---

## üõ†Ô∏è Libraries Used

- Python 3.x  
- NumPy, pandas, matplotlib  
- TensorFlow / Keras or PyTorch  
- scikit-learn (for preprocessing)

---

## üß† Learning Goals

- Understand the architecture and working of deep neural networks  
- Gain practical experience in building, training, and evaluating deep learning models  
- Explore real-world applications such as image classification, sequence modeling, and data generation

---


